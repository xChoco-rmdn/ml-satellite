
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook handles the model training process including:\n",
    "- Model architecture definition\n",
    "- Training loop implementation\n",
    "- Hyperparameter tuning\n",
    "- Model checkpointing\n",
    "- Training visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_preprocessed_data(data_dir='../processed_data'):\n",
    "    \"\"\"\n",
    "    Load preprocessed satellite image data\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing preprocessed data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Training, validation, and test datasets\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Load training data\n",
    "    X_train = np.load(data_dir / 'X_train.npy')\n",
    "    y_train = np.load(data_dir / 'y_train.npy')\n",
    "    \n",
    "    # Load validation data\n",
    "    X_val = np.load(data_dir / 'X_val.npy')\n",
    "    y_val = np.load(data_dir / 'y_val.npy')\n",
    "    \n",
    "    # Load test data\n",
    "    X_test = np.load(data_dir / 'X_test.npy')\n",
    "    y_test = np.load(data_dir / 'y_test.npy')\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    unique_labels = np.unique(np.concatenate([y_train, y_val, y_test]))\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    y_train_cat = np.array([label_to_index[label] for label in y_train])\n",
    "    y_val_cat = np.array([label_to_index[label] for label in y_val])\n",
    "    y_test_cat = np.array([label_to_index[label] for label in y_test])\n",
    "    \n",
    "    y_train_one_hot = tf.keras.utils.to_categorical(y_train_cat)\n",
    "    y_val_one_hot = tf.keras.utils.to_categorical(y_val_cat)\n",
    "    y_test_one_hot = tf.keras.utils.to_categorical(y_test_cat)\n",
    "    \n",
    "    return (\n",
    "        X_train, y_train_one_hot, \n",
    "        X_val, y_val_one_hot, \n",
    "        X_test, y_test_one_hot, \n",
    "        unique_labels\n",
    "    )\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, unique_labels = load_preprocessed_data()\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Number of classes:\", len(unique_labels))\n",
    "print(\"Classes:\", unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_satellite_model(input_shape, num_classes, base_filters=32):\n",
    "    \"\"\"\n",
    "    Create a CNN model for satellite image classification\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input images\n",
    "        num_classes (int): Number of classification categories\n",
    "        base_filters (int): Base number of filters in conv layers\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled classification model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(base_filters, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(base_filters * 2, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(base_filters * 4, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_satellite_model(\n",
    "    input_shape=X_train.shape[1:], \n",
    "    num_classes=len(unique_labels)\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create log directory for TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define callbacks\n",
    "tensorboard_callback = callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1, \n",
    "    profile_batch=0\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_accuracy', \n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Learning rate reducer\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks_list = [\n",
    "    tensorboard_callback,\n",
    "    early_stopping,\n",
    "    model_checkpoint,\n",
    "    lr_reducer\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_training_metrics(history):\n",
    "    \"\"\"\n",
    "    Visualize training and validation metrics\n",
    "    \n",
    "    Args:\n",
    "        history (keras.callbacks.History): Training history\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training metrics\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_model_artifacts(model, unique_labels):\n",
    "    \"\"\"\n",
    "    Save model and associated metadata\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained model\n",
    "        unique_labels (array): Array of unique class labels\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = Path('../models')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save full model\n",
    "    model.save(output_dir / 'satellite_classifier.keras')\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(output_dir / 'model_weights.weights.h5')\n",
    "    \n",
    "    # Save class labels\n",
    "    pd.Series(unique_labels).to_csv(output_dir / 'class_labels.csv', index=False)\n",
    "    \n",
    "    print(\"Model artifacts saved successfully.\")\n",
    "\n",
    "# Save model and related artifacts\n",
    "save_model_artifacts(model, unique_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
