
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Image Preprocessing\n",
    "\n",
    "This notebook handles all preprocessing steps for satellite imagery data including:\n",
    "- Data loading and validation\n",
    "- Image normalization\n",
    "- Data augmentation\n",
    "- Feature extraction\n",
    "- Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import satpy\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "def load_satellite_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load satellite imagery data from the specified directory\n",
    "    \n",
    "    Args:\n",
    "        data_dir (Path): Directory containing satellite image files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Loaded satellite imagery data\n",
    "    \"\"\"\n",
    "    # Placeholder for actual data loading logic\n",
    "    # This might involve using satpy or other satellite data libraries\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for img_path in data_dir.glob('*.tif'):  # Adjust file extension as needed\n",
    "        try:\n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            images.append(img)\n",
    "            \n",
    "            # Extract label (example: filename-based labeling)\n",
    "            label = img_path.stem.split('_')[0]  # Adjust based on your naming convention\n",
    "            labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'images': np.array(images),\n",
    "        'labels': np.array(labels)\n",
    "    }\n",
    "\n",
    "# Load satellite data\n",
    "satellite_data = load_satellite_data(DATA_DIR)\n",
    "\n",
    "# Basic data exploration\n",
    "print(\"Total images:\", len(satellite_data['images']))\n",
    "print(\"Image shape:\", satellite_data['images'][0].shape)\n",
    "print(\"Unique labels:\", np.unique(satellite_data['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_images(images, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess satellite images\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): Input images\n",
    "        target_size (tuple): Desired output image size\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed images\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Resize image\n",
    "        resized = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        normalized = resized.astype('float32') / 255.0\n",
    "        \n",
    "        processed_images.append(normalized)\n",
    "    \n",
    "    return np.array(processed_images)\n",
    "\n",
    "def augment_images(images):\n",
    "    \"\"\"\n",
    "    Apply data augmentation techniques\n",
    "    \n",
    "    Args:\n",
    "        images (np.ndarray): Input images\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Augmented images\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Random horizontal flip\n",
    "        if np.random.rand() > 0.5:\n",
    "            img = cv2.flip(img, 1)\n",
    "        \n",
    "        # Random rotation\n",
    "        angle = np.random.uniform(-15, 15)\n",
    "        rows, cols = img.shape[:2]\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "        rotated = cv2.warpAffine(img, rotation_matrix, (cols, rows))\n",
    "        \n",
    "        augmented_images.append(rotated)\n",
    "    \n",
    "    return np.array(augmented_images)\n",
    "\n",
    "# Preprocess images\n",
    "processed_images = preprocess_images(satellite_data['images'])\n",
    "\n",
    "# Optional: Data augmentation\n",
    "augmented_images = augment_images(processed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    processed_images, \n",
    "    satellite_data['labels'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Validation set:\", X_val.shape)\n",
    "print(\"Test set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_preprocessed_data(X_train, X_val, X_test, y_train, y_val, y_test, output_dir):\n",
    "    \"\"\"\n",
    "    Save preprocessed data to numpy files\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_val, X_test: Image datasets\n",
    "        y_train, y_val, y_test: Label datasets\n",
    "        output_dir (Path): Directory to save preprocessed data\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save training data\n",
    "    np.save(output_dir / 'X_train.npy', X_train)\n",
    "    np.save(output_dir / 'y_train.npy', y_train)\n",
    "    \n",
    "    # Save validation data\n",
    "    np.save(output_dir / 'X_val.npy', X_val)\n",
    "    np.save(output_dir / 'y_val.npy', y_val)\n",
    "    \n",
    "    # Save test data\n",
    "    np.save(output_dir / 'X_test.npy', X_test)\n",
    "    np.save(output_dir / 'y_test.npy', y_test)\n",
    "    \n",
    "    print(\"Preprocessed data saved successfully.\")\n",
    "\n",
    "# Save preprocessed data\n",
    "save_preprocessed_data(\n",
    "    X_train, X_val, X_test, \n",
    "    y_train, y_val, y_test, \n",
    "    output_dir='../processed_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
