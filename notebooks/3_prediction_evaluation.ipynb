
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evaluation\n",
    "\n",
    "This notebook handles model evaluation and prediction including:\n",
    "- Model loading\n",
    "- Making predictions\n",
    "- Performance metrics calculation\n",
    "- Visualization of results\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import cv2\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model_and_labels(model_dir='../models'):\n",
    "    \"\"\"\n",
    "    Load trained model and class labels\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing model artifacts\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Loaded model, class labels\n",
    "    \"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = models.load_model(model_dir / 'satellite_classifier.keras')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        model = None\n",
    "    \n",
    "    # Load class labels\n",
    "    try:\n",
    "        class_labels = pd.read_csv(model_dir / 'class_labels.csv', header=None)[0].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading class labels: {e}\")\n",
    "        class_labels = None\n",
    "    \n",
    "    return model, class_labels\n",
    "\n",
    "# Load model and labels\n",
    "model, class_labels = load_model_and_labels()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(\"Classes:\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_test_data(data_dir='../processed_data'):\n",
    "    \"\"\"\n",
    "    Load preprocessed test data\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing preprocessed data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Test images, test labels, one-hot encoded labels\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test = np.load(data_dir / 'X_test.npy')\n",
    "    y_test_orig = np.load(data_dir / 'y_test.npy')\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_test_one_hot = tf.keras.utils.to_categorical(y_test_orig)\n",
    "    \n",
    "    return X_test, y_test_orig, y_test_one_hot\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test_orig, y_test_one_hot = load_test_data()\n",
    "\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Unique test labels:\", np.unique(y_test_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def make_predictions(model, X_test):\n",
    "    \"\"\"\n",
    "    Generate predictions on test data\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained classification model\n",
    "        X_test (np.ndarray): Test image data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Predicted probabilities, predicted classes\n",
    "    \"\"\"\n",
    "    # Generate prediction probabilities\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to class predictions\n",
    "    y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    return y_pred_proba, y_pred_classes\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba, y_pred_classes = make_predictions(model, X_test)\n",
    "\n",
    "print(\"Prediction shape:\", y_pred_classes.shape)\n",
    "print(\"Unique predicted classes:\", np.unique(y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_performance_metrics(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels\n",
    "        y_pred (np.ndarray): Predicted labels\n",
    "        class_labels (list): List of class labels\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Precision, Recall, F1-Score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nWeighted Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=class_labels\n",
    "    ))\n",
    "\n",
    "# Calculate metrics\n",
    "calculate_performance_metrics(y_test_orig, y_pred_classes, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_confusion_matrix(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Create and plot confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels\n",
    "        y_pred (np.ndarray): Predicted labels\n",
    "        class_labels (list): List of class labels\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        xticklabels=class_labels, \n",
    "        yticklabels=class_labels\n",
    "    )\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_sample_predictions(X_test, y_test_orig, y_pred_classes, class_labels, num_samples=9):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions\n",
    "    \n",
    "    Args:\n",
    "        X_test (np.ndarray): Test images\n",
    "        y_test_orig (np.ndarray): True labels\n",
    "        y_pred_classes (np.ndarray): Predicted labels\n",
    "        class_labels (list): List of class labels\n",
    "        num_samples (int): Number of samples to visualize\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(X_test[i])\n",
    "        \n",
    "        true_label = class_labels[y_test_orig[i]]\n",
    "        pred_label = class_labels[y_pred_classes[i]]\n",
    "        \n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        title = f\"True: {true_label}\\nPred: {pred_label}\"\n",
    "        \n",
    "        plt.title(title, color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "visualize_confusion_matrix(y_test_orig, y_pred_classes, class_labels)\n",
    "visualize_sample_predictions(X_test, y_test_orig, y_pred_classes, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_error_analysis(y_true, y_pred, y_pred_proba, class_labels):\n",
    "    \"\"\"\n",
    "    Conduct detailed error analysis\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels\n",
    "        y_pred (np.ndarray): Predicted labels\n",
    "        y_pred_proba (np.ndarray): Prediction probabilities\n",
    "        class_labels (list): List of class labels\n",
    "    \"\"\"\n",
    "    # Identify misclassified samples\n",
    "    misclassified_mask = y_true != y_pred\n",
    "    misclassified_indices = np.where(misclassified_mask)[0]\n",
    "    \n",
    "    # Print error summary\n",
    "    print(\"Error Analysis:\")\n",
    "    print(f\"Total Misclassified Samples: {len(misclassified_indices)} / {len(y_true)}\")\n",
    "    \n",
    "    # Detailed misclassification breakdown\n",
    "    misclassification_matrix = pd.crosstab(\n",
    "        pd.Series(y_true[misclassified_mask], name='True Label'),\n",
    "        pd.Series(y_pred[misclassified_mask], name='Predicted Label')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMisclassification Matrix:\")\n",
    "    print(misclassification_matrix)\n",
    "    \n",
    "    # Confidence analysis of misclassifications\n",
    "    misclassified_confidences = y_pred_proba[misclassified_mask].max(axis=1)\n",
    "    print(\"\\nMisclassification Confidence Statistics:\")\n",
    "    print(f\"Mean Confidence: {misclassified_confidences.mean():.4f}\")\n",
    "    print(f\"Median Confidence: {np.median(misclassified_confidences):.4f}\")\n",
    "    print(f\"Min Confidence: {misclassified_confidences.min():.4f}\")\n",
    "\n",
    "# Perform error analysis\n",
    "perform_error_analysis(y_test_orig, y_pred_classes, y_pred_proba, class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
