
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evaluation\n",
    "\n",
    "This notebook handles model prediction and evaluation using the trained model.\n",
    "It includes:\n",
    "- Loading trained model\n",
    "- Making predictions\n",
    "- Model evaluation\n",
    "- Visualization of results\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.components.model import CloudNowcastingModel\n",
    "from src.utils import evaluate_model\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "\n",
    "# Configure logging\n",
    "logger.info(\"Starting prediction and evaluation notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_test_data_and_model(data_dir='artifacts', model_dir='artifacts'):\n",
    "    \"\"\"\n",
    "    Load test data and trained model\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing test data\n",
    "        model_dir (str): Directory containing trained model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_test, y_test, loaded model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load test data\n",
    "        X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n",
    "        y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "        \n",
    "        # Load trained model\n",
    "        model_path = os.path.join(model_dir, 'best_model.h5')\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        logger.info(\"Test data and model loaded successfully\")\n",
    "        return X_test, y_test, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load test data or model: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Load data and model\n",
    "X_test, y_test, model = load_test_data_and_model()\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def make_predictions(model, X_test):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained model\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained model\n",
    "        X_test (np.ndarray): Test input data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Predictions and prediction uncertainties\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize model for prediction\n",
    "        model_trainer = CloudNowcastingModel()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions, prediction_std = model_trainer.predict(model, X_test)\n",
    "        \n",
    "        logger.info(\"Predictions generated successfully\")\n",
    "        return predictions, prediction_std\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction generation failed: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Generate predictions\n",
    "predictions, prediction_std = make_predictions(model, X_test)\n",
    "\n",
    "print(\"Predictions completed\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Prediction uncertainty shape: {prediction_std.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model_performance(y_test, predictions):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using various metrics\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.ndarray): True test labels\n",
    "        predictions (np.ndarray): Model predictions\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate evaluation metrics\n",
    "        metrics = evaluate_model(y_test, predictions)\n",
    "        \n",
    "        logger.info(\"Model evaluation completed\")\n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model evaluation failed: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model_performance(y_test, predictions)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_predictions(X, y_true, y_pred, output_dir=None, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize input sequences, true targets, and predictions\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input sequences\n",
    "        y_true (np.ndarray): True target values\n",
    "        y_pred (np.ndarray): Predicted values\n",
    "        output_dir (str, optional): Directory to save plot\n",
    "        num_samples (int): Number of samples to plot\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4*num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Plot input sequence\n",
    "            axes[i, 0].imshow(X[i, -1, :, :, 0], cmap='viridis')\n",
    "            axes[i, 0].set_title(f'Input Sequence (t={i})')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Plot true target\n",
    "            axes[i, 1].imshow(y_true[i, 0, :, :, 0], cmap='viridis')\n",
    "            axes[i, 1].set_title(f'True Target (t={i+1})')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Plot prediction\n",
    "            axes[i, 2].imshow(y_pred[i, 0, :, :, 0], cmap='viridis')\n",
    "            axes[i, 2].set_title(f'Prediction (t={i+1})')\n",
    "            axes[i, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save or show plot\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(output_dir, 'prediction_comparison.png'))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        logger.info(\"Prediction visualization completed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction visualization failed: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Plot sample predictions\n",
    "plot_predictions(X_test, y_test, predictions, output_dir='artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_prediction_errors(y_true, y_pred, prediction_std, output_dir=None):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): True target values\n",
    "        y_pred (np.ndarray): Predicted values\n",
    "        prediction_std (np.ndarray): Prediction uncertainties\n",
    "        output_dir (str, optional): Directory to save analysis plot\n",
    "    \n",
    "    Returns:\n",
    "        dict: Error statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate absolute errors\n",
    "        abs_errors = np.abs(y_true - y_pred)\n",
    "        \n",
    "        # Prepare plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Error distribution\n",
    "        axes[0].hist(abs_errors.flatten(), bins=50, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('Distribution of Absolute Errors')\n",
    "        axes[0].set_xlabel('Absolute Error')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Error vs Uncertainty scatter\n",
    "        scatter = axes[1].scatter(\n",
    "            prediction_std.flatten(), \n",
    "            abs_errors.flatten(), \n",
    "            alpha=0.1, \n",
    "            c=prediction_std.flatten(), \n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1].set_title('Error vs Prediction Uncertainty')\n",
    "        axes[1].set_xlabel('Prediction Uncertainty')\n",
    "        axes[1].set_ylabel('Absolute Error')\n",
    "        plt.colorbar(scatter, ax=axes[1], label='Uncertainty')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save or show plot\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(output_dir, 'error_analysis.png'))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        # Compute error statistics\n",
    "        error_stats = {\n",
    "            'mean_absolute_error': np.mean(abs_errors),\n",
    "            'median_absolute_error': np.median(abs_errors),\n",
    "            'percentile_95_error': np.percentile(abs_errors, 95),\n",
    "            'max_error': np.max(abs_errors),\n",
    "            'error_std': np.std(abs_errors)\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Error analysis completed\")\n",
    "        return error_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analysis failed: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Analyze prediction errors\n",
    "error_stats = analyze_prediction_errors(y_test, predictions, prediction_std, output_dir='artifacts')\n",
    "\n",
    "print(\"\\nError Statistics:\")\n",
    "for stat_name, value in error_stats.items():\n",
    "    print(f\"{stat_name.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_evaluation_results(metrics, error_stats, predictions, prediction_std, output_dir='artifacts'):\n",
    "    \"\"\"\n",
    "    Save evaluation metrics, error statistics, and predictions\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Model evaluation metrics\n",
    "        error_stats (dict): Error analysis statistics\n",
    "        predictions (np.ndarray): Model predictions\n",
    "        prediction_std (np.ndarray): Prediction uncertainties\n",
    "        output_dir (str): Directory to save results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save evaluation metrics\n",
    "        metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "        metrics_df.to_csv(os.path.join(output_dir, 'evaluation_metrics.csv'), index=False)\n",
    "        \n",
    "        # Save error statistics\n",
    "        error_stats_df = pd.DataFrame([error_stats])\n",
    "        error_stats_df.to_csv(os.path.join(output_dir, 'error_statistics.csv'), index=False)\n",
    "        \n",
    "        # Save predictions\n",
    "        np.save(os.path.join(output_dir, 'predictions.npy'), predictions)\n",
    "        np.save(os.path.join(output_dir, 'prediction_uncertainty.npy'), prediction_std)\n",
    "        \n",
    "        logger.info(\"Evaluation results saved successfully\")\n",
    "        print(\"Evaluation results saved in artifacts directory\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save evaluation results: {e}\")\n",
    "        raise CustomException(e, sys)\n",
    "\n",
    "# Save evaluation results\n",
    "save_evaluation_results(metrics, error_stats, predictions, prediction_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
